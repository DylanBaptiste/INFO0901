{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import *\n",
    "\n",
    "# Setup Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.activations import *\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "# Setup numpy, pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "DEFAULT_W, DEFAULT_H = (16, 9)\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [DEFAULT_W, DEFAULT_H]\n",
    "matplotlib.rcParams[\"font.size\"] = 15\n",
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Load data\n",
    "FILE_NAME = \"winequality_white.csv\"\n",
    "Y_COL_NAME = \"quality\"\n",
    "data = pd.read_csv(FILE_NAME, sep=\";\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - distribution\n",
    "# - (FAIT) matrice de correlation (pearson correlation)\n",
    "# std, mean, boite moustache etc...\n",
    "# distribution par note\n",
    "\n",
    "data.describe()\n",
    "\n",
    "corr = data.corr(method='pearson')\n",
    "display(corr.style.background_gradient(cmap='coolwarm').set_precision(2))\n",
    "# classement:\n",
    "# top = abs(corr.loc[Y_COL_NAME]).sort_values(ascending=False)\n",
    "top = corr.loc[Y_COL_NAME][corr.index != Y_COL_NAME]\n",
    "sorted = abs(top).sort_values(ascending=False)\n",
    "display(top) # en abs car -1 donne une bonne correlation aussi (correlation negative)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x=sorted.index, height=top[sorted.index], width=1, edgecolor='black')\n",
    "ax.set_title(\"Classement des correlations des variables par rapport à la variable \\\"quality\\\"\", fontdict={\"size\":25})\n",
    "for i, v in enumerate(top[sorted.index].values):\n",
    "    ax.text(i - 0.25, (v + np.sign(v) * 0.015) - 0.01, f\"{round(v, 2):.2f}\", color='black')\n",
    "# plt.axhline(1, linestyle='--')\n",
    "# plt.axhline(-1, linestyle='--')\n",
    "plt.ylabel(\"Pearson correlation\", fontweight='light', fontsize='x-large')\n",
    "plt.xticks(rotation=67.5, horizontalalignment='right', fontweight='light', fontsize='large')\n",
    "plt.yticks(fontweight='light', fontsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CLASS = 10\n",
    "\n",
    "X = data.loc[:, data.columns != Y_COL_NAME]\n",
    "# X = ((X - X.mean()) / X.std()).values # Standardization colonne par colonne # (x - µ) / σ\n",
    "# ou\n",
    "X = ((X - X.min()) / (X.max() - X.min())).values # min-max scaling => 0 - 1 # (x - min) / (max - min)\n",
    "# Jmais compris c'etait quoi le mieux : https://kharshit.github.io/blog/2018/03/23/scaling-vs-normalization\n",
    "\n",
    "Y = data.loc[:, Y_COL_NAME].values\n",
    "Y = Y.reshape(-1, 1) # Y est scale entre 0 et 1 au moment de faire la regression (on garde les notes en entier pour le model de classif)\n",
    "\n",
    "NB_INPUT = X.shape[1] # nombre de variable en input des modeles\n",
    "NB_DATA = X.shape[0]\n",
    "# X, Y, X.shape, Y.shape\n",
    "\n",
    "# Split train/test/validation\n",
    "pTrain, pTest, pValidation = (0.80, 0.10, 0.10)\n",
    "assert (pTrain + pTest + pValidation) == 1.0, f\"La somme doit etre equel à 1, {pTrain, pTest, pValidation}\"\n",
    "SPLIT_TRAIN = int(NB_DATA * pTrain)\n",
    "SPLIT_TEST = int(NB_DATA * pTest)\n",
    "SPLIT_VAL = int(NB_DATA * pValidation)\n",
    "print(SPLIT_TRAIN, SPLIT_TEST, SPLIT_VAL)\n",
    "X_train, X_test, X_val = X[:SPLIT_TRAIN], X[SPLIT_TRAIN:SPLIT_TRAIN+SPLIT_TEST], X[-SPLIT_VAL:]\n",
    "Y_train, Y_test, Y_val = Y[:SPLIT_TRAIN], Y[SPLIT_TRAIN:SPLIT_TRAIN+SPLIT_TEST], Y[-SPLIT_VAL:]\n",
    "print(X_train.shape, X_test.shape, X_val.shape)\n",
    "print(Y_train.shape, Y_test.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model():\n",
    "\tinput = Input(shape=(NB_INPUT,), name=\"input\")\n",
    "\tx = Dense(1000, activation=\"relu\")(input)\n",
    "\tx = Dropout(0.2)(x)\n",
    "\tx = BatchNormalization()(x)\n",
    "\tx = Dense(500, activation=\"relu\")(x)\n",
    "\tx = Dropout(0.2)(x)\n",
    "\tx = BatchNormalization()(x)\n",
    "\tx = Dense(250, activation=\"relu\")(x)\n",
    "\tx = Dropout(0.2)(x)\n",
    "\tx = BatchNormalization()(x)\n",
    "\t# output = Dense(1, activation=ReLU(max_value=1.0), name=\"prediction\")(x)\n",
    "\toutput = Dense(1, activation=\"sigmoid\", name=\"prediction\")(x)\n",
    "\n",
    "\treturn Model(input, output, name=\"regression_model\")\n",
    "\n",
    "def classification_model():\n",
    "\tinput = Input(shape=(NB_INPUT,), name=\"input\")\n",
    "\tx = Dense(1000, activation=\"relu\")(input)\n",
    "\tx = Dropout(0.2)(x)\n",
    "\tx = BatchNormalization()(x)\n",
    "\tx = Dense(500, activation=\"relu\")(x)\n",
    "\tx = Dropout(0.2)(x)\n",
    "\tx = BatchNormalization()(x)\n",
    "\tx = Dense(250, activation=\"relu\")(x)\n",
    "\tx = Dropout(0.2)(x)\n",
    "\tx = BatchNormalization()(x)\n",
    "\toutput = Dense(NB_CLASS, activation=Softmax(), name=\"prediction\")(x)\n",
    "\t\n",
    "\treturn Model(input, output, name=\"classification_model\")\n",
    "\n",
    "def regression_accuracy(y_true, y_pred):\n",
    "\ty_true = tf.cast(tf.round(y_true * NB_CLASS), dtype=tf.int8)\n",
    "\ty_pred = tf.cast(tf.round(y_pred * NB_CLASS), dtype=tf.int8)\n",
    "\teq = tf.cast(tf.math.equal(y_true, y_pred), dtype=tf.float16)\n",
    "\treturn tf.reduce_mean(eq)\n",
    "\n",
    "# regression_model().summary()\n",
    "# classification_model().summary()\n",
    "\n",
    "# yt, yp = tf.constant([0,1,2,3,4,5,6,7,8,9]) / 10, tf.constant([0.05,0.0,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])\n",
    "# print(yt, yp)\n",
    "# regression_accuracy(yt, yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regModel = regression_model()\n",
    "regModel.compile(loss=\"mse\", metrics=[regression_accuracy], optimizer=Adam(learning_rate=0.0001))\n",
    "regModel.summary()\n",
    "\n",
    "hreg = regModel.fit(X_train, Y_train / NB_CLASS, validation_data=[X_test, Y_test / NB_CLASS], batch_size=32, epochs=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Loss\")\n",
    "plt.plot(hreg.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(hreg.history[\"val_loss\"], label=\"test loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(hreg.history[\"regression_accuracy\"], label=\"train accuracy\")\n",
    "plt.plot(hreg.history[\"val_regression_accuracy\"], label=\"test accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(hreg.history[\"val_loss\"]) / np.array(hreg.history[\"loss\"]), label=\"ratio test/train loss\")\n",
    "plt.axhline(y=1, color='b', linestyle='--', label=\"Equilibre\")\n",
    "plt.legend()\n",
    "# Pour evaluer le surapprentissage (plus on s'eloigne de 1 en positif plus on est en surapprentisssage)\n",
    "# si on est en dessous de 1 ca veut dire que le modele performe mieux sur le test que le train (c'est plutot normal vu qu'il y a du Dropout dans le model là)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classModel = classification_model()\n",
    "classModel.compile(loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=Adam(learning_rate=0.0001))\n",
    "classModel.summary()\n",
    "\n",
    "hclass = classModel.fit(X_train, Y_train, validation_data=[X_test, Y_test], batch_size=32, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(hclass.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(hclass.history[\"val_loss\"], label=\"test loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(hclass.history[\"accuracy\"], label=\"train accuracy\")\n",
    "plt.plot(hclass.history[\"val_accuracy\"], label=\"test accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(hclass.history[\"val_loss\"]) / np.array(hclass.history[\"loss\"]), label=\"ratio test/train loss\")\n",
    "plt.axhline(y=1, color='b', linestyle='--', label=\"Equilibre\")\n",
    "plt.legend()\n",
    "# Pour evaluer le surapprentissage (plus on s'eloigne de 1 en positif plus on est en surapprentisssage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred, Y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "Y_pred = np.argmax(classModel.predict(X_train), axis=1)\n",
    "Y_true = Y_train.reshape(-1,)\n",
    "cm = confusion_matrix(Y_true, Y_pred, labels=np.unique(Y_true))\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt='g')\n",
    "plt.show()\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt='.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.argmax(classModel.predict(X_test), axis=1)\n",
    "Y_true = Y_test.reshape(-1,)\n",
    "cm = confusion_matrix(Y_true, Y_pred, labels=np.unique(Y_true))\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt='g')\n",
    "plt.show()\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Cross Val pour test different meta parametre\n",
    "# plot les resultats + leur std par epochs\n",
    "\n",
    "# TODO tester l'accuracy de la classif en faisant apparaitre aussi le seconde choix du model\n",
    "# possible aussi de regarder si les deux premier choix sont \"proche\" (cad si une prediction à 5 en choix 1 est suivit par un 4 ou 6 en choix 2 (faisable en faaisant mean(abs(choix1-choix2)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Conclure\n",
    "# TODO souligner les limites du dataset: que 3 votants, manque d'element en input comme le prix, les labels de qualité etc pour que le model soit pertinent dans le cadre d'une utilisation de classification de vin en situation de vente reel"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d30491a918b2bb80bddb8df1507fb40d67bf864fa8abf51b7fc41f1e6d55db91"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
