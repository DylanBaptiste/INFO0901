{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import *\n",
    "\n",
    "# Setup Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.activations import *\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "# Setup numpy, pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "DEFAULT_W, DEFAULT_H = (16, 9)\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [DEFAULT_W, DEFAULT_H]\n",
    "matplotlib.rcParams[\"font.size\"] = 15\n",
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Load data\n",
    "FILE_NAME_RED = \"winequality-red.csv\"\n",
    "FILE_NAME_WHITE = \"winequality_white.csv\"\n",
    "Y_COL_NAME = \"quality\"\n",
    "\n",
    "# Merge for both datasets (red wines, white wines)\n",
    "data_red = pd.read_csv(FILE_NAME_RED, sep=\",\")\n",
    "data_red['type']='red'\n",
    "data_white = pd.read_csv(FILE_NAME_WHITE, sep=\";\")\n",
    "data_white['type']='white'\n",
    "data=pd.concat([data_red, data_white]).sample(frac=1).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Distribution et informations statistiques de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and Standard Pearson deviation\n",
    "\n",
    "print(data.describe())\n",
    "reds=data[data['type']=='red']\n",
    "whites=data[data['type']=='white']\n",
    "\n",
    "x_red, y_red=np.unique(reds['quality'], return_counts=True)\n",
    "x_white, y_white=np.unique(whites['quality'], return_counts=True)\n",
    "plt.figure(facecolor='#EAEEF5')\n",
    "\n",
    "width=0.4\n",
    "ax=plt.axes()\n",
    "ax.set_facecolor('#EAEEF5')\n",
    "plt.bar(x_red-0.2, y_red, width, color='red')\n",
    "plt.bar(x_white+0.2, y_white, width, color='white')\n",
    "plt.legend(['Vins rouges', 'Vins blancs'])\n",
    "plt.title('Distribution de la qualité des vins en fonction du type (rouge ou blanc)')\n",
    "\n",
    "# Boxplot for each variable grouped by the quality values\n",
    "columns=data.columns.drop(['quality', 'type', 'index'])\n",
    "\n",
    "for column in columns:\n",
    "    plt.figure()\n",
    "    data.boxplot(column=column, by=['quality', 'type'], grid=False)\n",
    "    plt.title('Boîte à moustache de la variable \"{}\" en fonction du type de vin et de la note obtenue'.format(column))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - distribution\n",
    "# - (FAIT) matrice de correlation (pearson correlation)\n",
    "# std, mean, boite moustache etc...\n",
    "# distribution par note\n",
    "\n",
    "corr = data.corr(method='pearson')\n",
    "display(corr.style.background_gradient(cmap='coolwarm').set_precision(2))\n",
    "# classement:\n",
    "# top = abs(corr.loc[Y_COL_NAME]).sort_values(ascending=False)\n",
    "top = corr.loc[Y_COL_NAME][corr.index != Y_COL_NAME]\n",
    "sorted = abs(top).sort_values(ascending=False)\n",
    "display(top) # en abs car -1 donne une bonne correlation aussi (correlation negative)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x=sorted.index, height=top[sorted.index], width=1, edgecolor='black')\n",
    "ax.set_title(\"Classement des correlations des variables par rapport à la variable \\\"quality\\\"\", fontdict={\"size\":25})\n",
    "for i, v in enumerate(top[sorted.index].values):\n",
    "    ax.text(i - 0.25, (v + np.sign(v) * 0.015) - 0.01, f\"{round(v, 2):.2f}\", color='black')\n",
    "# plt.axhline(1, linestyle='--')\n",
    "# plt.axhline(-1, linestyle='--')\n",
    "plt.ylabel(\"Pearson correlation\", fontweight='light', fontsize='x-large')\n",
    "plt.xticks(rotation=67.5, horizontalalignment='right', fontweight='light', fontsize='large')\n",
    "plt.yticks(fontweight='light', fontsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CLASS = 10\n",
    "\n",
    "X = data.drop([Y_COL_NAME, 'type'], axis=1)\n",
    "# X = ((X - X.mean()) / X.std()).values # Standardization colonne par colonne # (x - µ) / σ\n",
    "# ou\n",
    "# Normalisation min-max\n",
    "def min_max_scaling(X):\n",
    "    return (X - X.min()) / (X.max() - X.min())\n",
    "\n",
    "# Réduction et centrage des valeurs autour de la moyenne\n",
    "def norm_scaling(X):\n",
    "    return ((X - X.mean()) / X.std())\n",
    "\n",
    "X = min_max_scaling(X)\n",
    "\n",
    "Y = data.loc[:, Y_COL_NAME].values\n",
    "Y = Y.reshape(-1, 1) # Y est scale entre 0 et 1 au moment de faire la regression (on garde les notes en entier pour le model de classif)\n",
    "\n",
    "NB_INPUT = X.shape[1] # nombre de variable en input des modeles\n",
    "NB_DATA = X.shape[0]\n",
    "# X, Y, X.shape, Y.shape\n",
    "\n",
    "# Split train/test/validation\n",
    "pTrain, pTest, pValidation = (0.80, 0.10, 0.10)\n",
    "assert (pTrain + pTest + pValidation) == 1.0, f\"La somme doit etre equel à 1, {pTrain, pTest, pValidation}\"\n",
    "SPLIT_TRAIN = int(NB_DATA * pTrain)\n",
    "SPLIT_TEST = int(NB_DATA * pTest)\n",
    "SPLIT_VAL = int(NB_DATA * pValidation)\n",
    "print(SPLIT_TRAIN, SPLIT_TEST, SPLIT_VAL)\n",
    "X_train, X_test, X_val = X[:SPLIT_TRAIN], X[SPLIT_TRAIN:SPLIT_TRAIN+SPLIT_TEST], X[-SPLIT_VAL:]\n",
    "Y_train, Y_test, Y_val = Y[:SPLIT_TRAIN], Y[SPLIT_TRAIN:SPLIT_TRAIN+SPLIT_TEST], Y[-SPLIT_VAL:]\n",
    "print(X_train.shape, X_test.shape, X_val.shape)\n",
    "print(Y_train.shape, Y_test.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Modèles & Entrainments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.activations import *\n",
    "\n",
    "def classification_model(DROP_RATE = 0.1, RELU_ALPHA = 0.2, N_NEURONES = 32, N_LAYERS = 1):\n",
    "\t\n",
    "\tinput = Input(shape=(NB_INPUT,), name=\"input\")\n",
    "\tx = LayerNormalization()(input)\n",
    "\n",
    "\tfor _ in range(N_LAYERS):\n",
    "\t\tx = LeakyReLU(RELU_ALPHA)(BatchNormalization()(Dense(N_NEURONES)(x)))\n",
    "\t\tx = Dropout(DROP_RATE)(x)\n",
    "\t\n",
    "\toutput = Dense(NB_CLASS, activation=Softmax(), name=\"prediction\")(x)\n",
    "\t\n",
    "\treturn Model(input, output, name=\"classification_model\")\n",
    "\n",
    "epochs = 50\n",
    "lr = 1e-2\n",
    "batch_size = 32\n",
    "\n",
    "classModel = classification_model(DROP_RATE = 0.2, RELU_ALPHA = 0.2, N_NEURONES = 32, N_LAYERS = 3)\n",
    "classModel.compile(loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=Adam(lr))\n",
    "classModel.summary()\n",
    "\n",
    "hclass = classModel.fit(X_train, Y_train, validation_data=[X_test, Y_test], batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 / 10**(epoch/50) ) # \n",
    "# hclass = classModel.fit(X_train, Y_train, validation_data=[X_test, Y_test], batch_size=32, epochs=200, callbacks=[lr_schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "\n",
    "def logistic_regression_model(random_state=0, \n",
    "                     multi_class='ovr', \n",
    "                     solver='liblinear', \n",
    "                     max_iter=100):\n",
    "    \n",
    "    return LogisticRegression(random_state=random_state, \n",
    "                              multi_class=multi_class, \n",
    "                              solver=solver, \n",
    "                              max_iter=max_iter)\n",
    "\n",
    "def ridge_classification_model():\n",
    "    return RidgeClassifier()\n",
    "\n",
    "\n",
    "logRegModel=logistic_regression_model(0, 'multinomial', 'lbfgs', 1000)\n",
    "logRegModel.fit(X_train, Y_train.reshape(-1))\n",
    "print(logRegModel.predict_log_proba(X_test))\n",
    "print(logRegModel.score(X_val, Y_val))\n",
    "\n",
    "ridgeClf=ridge_classification_model()\n",
    "ridgeClf.fit(X_train, Y_train.reshape(-1))\n",
    "ridgeClf.score(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(hclass.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(hclass.history[\"val_loss\"], label=\"test loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(hclass.history[\"accuracy\"], label=\"train accuracy\")\n",
    "plt.plot(hclass.history[\"val_accuracy\"], label=\"test accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(hclass.history[\"val_loss\"]) / np.array(hclass.history[\"loss\"]), label=\"ratio test/train loss\")\n",
    "plt.axhline(y=1, color='b', linestyle='--', label=\"Equilibre\")\n",
    "plt.legend()\n",
    "# Pour evaluer le surapprentissage (plus on s'eloigne de 1 en positif plus on est en surapprentisssage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "Y_pred = np.argmax(classModel.predict(X_train), axis=1)\n",
    "Y_true = Y_train.reshape(-1,)\n",
    "cm = confusion_matrix(Y_true, Y_pred, labels=np.arange(0, 10))\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt='g')\n",
    "plt.show()\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt='.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.argmax(classModel.predict(X_test), axis=1)\n",
    "Y_true = Y_test.reshape(-1,)\n",
    "cm = confusion_matrix(Y_true, Y_pred, labels=np.arange(0, 10))\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt='g')\n",
    "plt.show()\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Cross Val pour test different meta parametre\n",
    "# plot les resultats + leur std par epochs\n",
    "\n",
    "# TODO tester l'accuracy de la classif en faisant apparaitre aussi le seconde choix du model\n",
    "# possible aussi de regarder si les deux premier choix sont \"proche\" (cad si une prediction à 5 en choix 1 est suivit par un 4 ou 6 en choix 2 (faisable en faaisant mean(abs(choix1-choix2)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Conclure\n",
    "# TODO souligner les limites du dataset: que 3 votants, manque d'element en input comme le prix, les labels de qualité etc pour que le model soit pertinent dans le cadre d'une utilisation de classification de vin en situation de vente reel"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d30491a918b2bb80bddb8df1507fb40d67bf864fa8abf51b7fc41f1e6d55db91"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
