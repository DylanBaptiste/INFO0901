{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import *\n",
    "\n",
    "# Setup Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.activations import *\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "# Setup numpy, pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "DEFAULT_W, DEFAULT_H = (16, 9)\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [DEFAULT_W, DEFAULT_H]\n",
    "matplotlib.rcParams[\"font.size\"] = 15\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "# Load data\n",
    "FILE_NAME_RED = \"winequality-red.csv\"\n",
    "FILE_NAME_WHITE = \"winequality_white.csv\"\n",
    "Y_COL_NAME = \"quality\"\n",
    "\n",
    "# Merge for both datasets (red wines, white wines)\n",
    "data_red = pd.read_csv(FILE_NAME_RED, sep=\",\")\n",
    "data_red.insert(11, \"type\", \"red\")\n",
    "\n",
    "data_white = pd.read_csv(FILE_NAME_WHITE, sep=\";\")\n",
    "data_white.insert(11, \"type\", \"white\")\n",
    "\n",
    "data = pd.concat([data_red, data_white]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Statistiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Distribution et informations statistiques de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and Standard Pearson deviation\n",
    "\n",
    "print(data.describe())\n",
    "reds = data[data[\"type\"]==\"red\"]\n",
    "whites = data[data[\"type\"]==\"white\"]\n",
    "\n",
    "x_red, y_red = np.unique(reds[\"quality\"], return_counts=True)\n",
    "x_white, y_white = np.unique(whites[\"quality\"], return_counts=True)\n",
    "plt.figure(facecolor=\"#EAEEF5\")\n",
    "\n",
    "width = 0.4\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"#EAEEF5\")\n",
    "plt.bar(x_red - 0.2, y_red, width, color=\"red\")\n",
    "plt.bar(x_white + 0.2, y_white, width, color=\"white\")\n",
    "plt.legend([\"Vins rouges\", \"Vins blancs\"])\n",
    "plt.title(\"Distribution de la qualité des vins en fonction du type (rouge ou blanc)\")\n",
    "\n",
    "# Boxplot for each variable grouped by the quality values, removing qualitative variable \"type\" and the dataframe index\n",
    "columns = data.columns.drop([\"quality\", \"type\"])\n",
    "\n",
    "for column in columns:\n",
    "    plt.figure()\n",
    "    data.boxplot(column=column, by=[\"quality\", \"type\"], grid=False)\n",
    "    plt.title(\"Boîte à moustache de la variable \\\"{}\\\" en fonction du type de vin et de la note obtenue\".format(column))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO\n",
    "# # - distribution\n",
    "# # - (FAIT) matrice de correlation (pearson correlation)\n",
    "# # std, mean, boite moustache etc...\n",
    "# # distribution par note\n",
    "\n",
    "# corr = data.corr(method=\"pearson\")\n",
    "# # display(corr.style.background_gradient(cmap=\"coolwarm\"))\n",
    "# # classement:\n",
    "# # top = abs(corr.loc[Y_COL_NAME]).sort_values(ascending=False)\n",
    "# top = corr.loc[Y_COL_NAME][corr.index != Y_COL_NAME]\n",
    "# sorted = abs(top).sort_values(ascending=False)\n",
    "# display(top) # en abs car -1 donne une bonne correlation aussi (correlation negative)\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.bar(x=sorted.index, height=top[sorted.index], width=1, edgecolor=\"black\")\n",
    "# ax.set_title(\"Classement des correlations des variables par rapport à la variable \\\"quality\\\"\", fontdict={\"size\":25})\n",
    "# for i, v in enumerate(top[sorted.index].values):\n",
    "#     ax.text(i - 0.25, (v + np.sign(v) * 0.015) - 0.01, f\"{round(v, 2):.2f}\", color=\"black\")\n",
    "# # plt.axhline(1, linestyle=\"--\")\n",
    "# # plt.axhline(-1, linestyle=\"--\")\n",
    "# plt.ylabel(\"Pearson correlation\", fontweight=\"light\", fontsize=\"x-large\")\n",
    "# plt.xticks(rotation=67.5, horizontalalignment=\"right\", fontweight=\"light\", fontsize=\"large\")\n",
    "# plt.yticks(fontweight=\"light\", fontsize=\"small\")\n",
    "# plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "correlations = data.corr()\n",
    "\n",
    "sns.heatmap(correlations, annot=True, linewidths=.2, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Colinéarité des variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Normalisation des données et encodage des variables qualitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "\n",
    "# La colonne type va être encodée pour qu\"elle puisse être comprise dans l\"analyse\n",
    "encoder = ce.OneHotEncoder(cols=\"type\", handle_unknown=\"return_nan\", return_df=True, use_cat_names=True)\n",
    "\n",
    "data_tf = encoder.fit_transform(data)\n",
    "\n",
    "# X = ((X - X.mean()) / X.std()).values # Standardization colonne par colonne # (x - µ) / σ\n",
    "# ou\n",
    "# Normalisation min-max\n",
    "def min_max_scaling(X):\n",
    "    return (X - X.min()) / (X.max() - X.min())\n",
    "\n",
    "# Réduction et centrage des valeurs autour de la moyenne\n",
    "def norm_scaling(X):\n",
    "    return ((X - X.mean()) / X.std())\n",
    "\n",
    "X = data_tf.iloc[:, :-1]\n",
    "\n",
    "X = min_max_scaling(X)\n",
    "\n",
    "Y = data_tf.loc[:, Y_COL_NAME].values\n",
    "Y = Y.reshape(-1, 1) # Y est scale entre 0 et 1 au moment de faire la regression (on garde les notes en entier pour le model de classif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Etude et limitation de la colinéarité des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calc_vif(X):\n",
    "\n",
    "    # Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    return(vif)\n",
    "\n",
    "def fix_multicollinearity(X, threshold=10):\n",
    "    # Calcul de la VIF initiale\n",
    "    vif = calc_vif(X)\n",
    "    \n",
    "    # Tri des VIF dans l'ordre décroissant\n",
    "    sorted_vif = vif.sort_values(axis=0, by=\"VIF\", ascending=False, ignore_index=True)\n",
    "    \n",
    "    # Extraction de la VIF la plus importante\n",
    "    vif_top = sorted_vif.loc[0, \"VIF\"]\n",
    "    \n",
    "    # On mémorise le dataframe modifié X_mod, et on l'initialise à X\n",
    "    X_mod = X\n",
    "    \n",
    "    # Tant que la VIF la plus grande est au-dessus du seuil\n",
    "    while vif_top > threshold:\n",
    "        # Suppression de la variable ayant la VIF la plus haute\n",
    "        columns = sorted_vif.loc[1:, \"variables\"]\n",
    "        \n",
    "        # Extraction des colonnes n-1 colonnes restantes du dataframe, et on écrase le dataframe stocké dans X_mod\n",
    "        X_mod = X_mod[columns]\n",
    "        \n",
    "        # Calcul de la VIF pour chaque variable\n",
    "        vif = calc_vif(X_mod)\n",
    "    \n",
    "        # Tri des VIF dans l'ordre décroissant\n",
    "        sorted_vif = vif.sort_values(axis=0, by=\"VIF\", ascending=False, ignore_index=True)\n",
    "        \n",
    "        # Extraction de la VIF la plus importante\n",
    "        vif_top = sorted_vif.loc[0, \"VIF\"]\n",
    "    \n",
    "    # La fonction retourne le dataframe sans les variables redondantes\n",
    "    return X_mod\n",
    "        \n",
    "    \n",
    "\n",
    "X = fix_multicollinearity(X, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CLASS = len(np.unique(Y))\n",
    "NB_INPUT = X.shape[1] # nombre de variable en input des modeles\n",
    "NB_DATA = X.shape[0]\n",
    "# X, Y, X.shape, Y.shape\n",
    "\n",
    "# Split train/test\n",
    "X, X_test, Y, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "X, X_test, Y, Y_test = X.values, X_test.values, Y - 3, Y_test - 3\n",
    "# Y_train = Y_train.reshape(-1)\n",
    "# Y_test = Y_test.reshape(-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Modèles & Entrainments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Définition des paramètres généraux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CLASS = len(np.unique(Y))\n",
    "NB_INPUT = X.shape[1] # nombre de variable en input des modeles\n",
    "NB_DATA = X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Définition du réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.activations import *\n",
    "\n",
    "def classification_model(DROP_RATE = 0.1, RELU_ALPHA = 0.2, N_NEURONES = 32, N_LAYERS = 1):\n",
    "\t\n",
    "\tinput = Input(shape=(NB_INPUT,), name=\"input\")\n",
    "\tx = LayerNormalization()(input)\n",
    "\n",
    "\tfor _ in range(N_LAYERS):\n",
    "\t\tx = LeakyReLU(RELU_ALPHA)(BatchNormalization()(Dense(N_NEURONES)(x)))\n",
    "\t\tx = Dropout(DROP_RATE)(x)\n",
    "\t\n",
    "\toutput = Dense(NB_CLASS, activation=Softmax(), name=\"prediction\")(x)\n",
    "\t\n",
    "\treturn Model(input, output, name=\"classification_model\")\n",
    "\n",
    "# NOTE: Entrainement fait dans la CV plus bas\n",
    "# epochs = 50\n",
    "# lr = 1e-2\n",
    "# batch_size = 32\n",
    "\n",
    "# classModel = classification_model(DROP_RATE = 0.2, RELU_ALPHA = 0.2, N_NEURONES = 32, N_LAYERS = 3)\n",
    "# classModel.compile(loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=Adam(lr))\n",
    "# classModel.summary()\n",
    "#\n",
    "# hclass = classModel.fit(X, Y, validation_data=[X_test, Y_test], batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 / 10**(epoch/50) ) # \n",
    "# hclass = classModel.fit(X_train, Y_train, validation_data=[X_test, Y_test], batch_size=32, epochs=200, callbacks=[lr_schedule])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Définition des modèles de régression/classification et de la fonction d\"entraîntement par KFold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_validate, StratifiedKFold\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Ridge\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\"\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\"),\n",
    "    SVC(),\n",
    "    RidgeClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    MLPClassifier(max_iter=1500),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB()\n",
    "]\n",
    "\n",
    "def apply_model(model, X_train, X_test, Y_train, Y_test):\n",
    "\tmodel.fit(X_train, Y_train)\n",
    "\tprint(\"Précision du modèle : %0.2f\" % model.score(X_test, Y_test))\n",
    "\treturn model\n",
    "    \n",
    "def apply_model_cv(model, X_train, Y_train, cv, scoring=None):\n",
    "    scores = cross_validate(model, X_train, Y_train, cv=cv, scoring=scoring)\n",
    "    print(scores)\n",
    "\t# print(f\"\"\"\n",
    "\t# accuracy: {np.round(scores*100, 2)}\n",
    "\t# mean: {np.mean(scores*100):.2f}%\n",
    "\t# ecart-type: {np.std(scores*100):.2f}%\n",
    "\t# \"\"\")\n",
    "        \n",
    "# ######\n",
    "# import sklearn\n",
    "# sklearn.metrics.SCORERS.keys()\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Application des modèles et résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Evaluation des modèles de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "for i in range(len(classifiers)):\n",
    "    clf = classifiers[i]\n",
    "    print(f\"*** Modèle : {names[i]} ***\")\n",
    "    \n",
    "    start = time()\n",
    "    apply_model_cv(clf, X, Y.reshape(-1), cv, [\"f1_macro\", \"accuracy\"])\n",
    "    end = time()\n",
    "    print(f\"Temps d'exécution : {np.around(end - start, 1)}s\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Evaluation du réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 1e-2\n",
    "batch_size = 32\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "hclass = []\n",
    "for fi, (train_i, val_i) in enumerate(cv.split(X, Y)):\n",
    "\tx_train, y_train = X[train_i], Y[train_i]\n",
    "\tx_val, y_val = X[val_i], Y[val_i]\n",
    "\n",
    "\tclassModel = classification_model(DROP_RATE = 0.2, RELU_ALPHA = 0.2, N_NEURONES = 32, N_LAYERS = 3)\n",
    "\tclassModel.compile(loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=Adam(lr))\n",
    "\thclass.append(classModel.fit(x_train, y_train, validation_data=[x_val, y_val], batch_size=batch_size, epochs=epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(hist, name=\"loss\"):\n",
    "\treturn np.array([h.history[name] for h in hist])\n",
    "loss = get_metrics(hclass, \"loss\")\n",
    "val_loss = get_metrics(hclass, \"val_loss\")\n",
    "# plt.plot(np.mean(loss, axis=0))\n",
    "# plt.plot(np.mean(val_loss, axis=0))\n",
    "plt.errorbar(x=range(loss.shape[1]), y=np.mean(loss, axis=0), yerr=np.std(loss, axis=0), label=\"loss\", fmt=\"o-\")\n",
    "plt.errorbar(x=range(val_loss.shape[1]), y=np.mean(val_loss, axis=0), yerr=np.std(val_loss, axis=0), label=\"val_loss\", fmt=\"o-\")\n",
    "plt.title(\"loss vs val_loss, CV: 5 folds\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.argmax(classModel.predict(X), axis=1)\n",
    "Y_true = Y\n",
    "cm = confusion_matrix(Y_true, Y_pred, labels=np.arange(0, 10))\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt=\"g\")\n",
    "plt.show()\n",
    "cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.argmax(classModel.predict(X_test), axis=1)\n",
    "Y_true = Y_test\n",
    "cm = confusion_matrix(Y_true, Y_pred, labels=np.arange(0, 10))\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt=\"g\")\n",
    "plt.show()\n",
    "cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Cross Val pour test different meta parametre\n",
    "# plot les resultats + leur std par epochs\n",
    "\n",
    "# TODO tester l\"accuracy de la classif en faisant apparaitre aussi le second choix du model\n",
    "# possible aussi de regarder si les deux premier choix sont \"proche\" (cad si une prediction à 5 en choix 1 est suivit par un 4 ou 6 en choix 2 (faisable en faaisant mean(abs(choix1-choix2)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Conclure\n",
    "# TODO souligner les limites du dataset: que 3 votants, manque d\"element en input comme le prix, les labels de qualité etc pour que le model soit pertinent dans le cadre d\"une utilisation de classification de vin en situation de vente reel"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d30491a918b2bb80bddb8df1507fb40d67bf864fa8abf51b7fc41f1e6d55db91"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
