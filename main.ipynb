{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import *\n",
    "\n",
    "# Setup Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.activations import *\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "# Setup numpy, pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "DEFAULT_W, DEFAULT_H = (16, 9)\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [DEFAULT_W, DEFAULT_H]\n",
    "matplotlib.rcParams[\"font.size\"] = 15\n",
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Load data\n",
    "FILE_NAME_RED = \"winequality-red.csv\"\n",
    "FILE_NAME_WHITE = \"winequality_white.csv\"\n",
    "Y_COL_NAME = \"quality\"\n",
    "\n",
    "# Merge for both datasets (red wines, white wines)\n",
    "data_red = pd.read_csv(FILE_NAME_RED, sep=\",\")\n",
    "data_red['type']='red'\n",
    "data_white = pd.read_csv(FILE_NAME_WHITE, sep=\";\")\n",
    "data_white['type']='white'\n",
    "data = pd.concat([data_red, data_white]).sample(frac=1).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Distribution et informations statistiques de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and Standard Pearson deviation\n",
    "\n",
    "print(data.describe())\n",
    "reds = data[data['type']=='red']\n",
    "whites = data[data['type']=='white']\n",
    "\n",
    "x_red, y_red = np.unique(reds['quality'], return_counts=True)\n",
    "x_white, y_white = np.unique(whites['quality'], return_counts=True)\n",
    "plt.figure(facecolor='#EAEEF5')\n",
    "\n",
    "width=0.4\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor('#EAEEF5')\n",
    "plt.bar(x_red-0.2, y_red, width, color='red')\n",
    "plt.bar(x_white+0.2, y_white, width, color='white')\n",
    "plt.legend(['Vins rouges', 'Vins blancs'])\n",
    "plt.title('Distribution de la qualité des vins en fonction du type (rouge ou blanc)')\n",
    "\n",
    "# Boxplot for each variable grouped by the quality values, removing qualitative variable 'type' and the dataframe index\n",
    "columns = data.columns.drop(['quality', 'type', 'index'])\n",
    "\n",
    "for column in columns:\n",
    "    plt.figure()\n",
    "    data.boxplot(column=column, by=['quality', 'type'], grid=False)\n",
    "    plt.title('Boîte à moustache de la variable \"{}\" en fonction du type de vin et de la note obtenue'.format(column))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - distribution\n",
    "# - (FAIT) matrice de correlation (pearson correlation)\n",
    "# std, mean, boite moustache etc...\n",
    "# distribution par note\n",
    "\n",
    "corr = data.corr(method='pearson')\n",
    "display(corr.style.background_gradient(cmap='coolwarm').set_precision(2))\n",
    "# classement:\n",
    "# top = abs(corr.loc[Y_COL_NAME]).sort_values(ascending=False)\n",
    "top = corr.loc[Y_COL_NAME][corr.index != Y_COL_NAME]\n",
    "sorted = abs(top).sort_values(ascending=False)\n",
    "display(top) # en abs car -1 donne une bonne correlation aussi (correlation negative)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x=sorted.index, height=top[sorted.index], width=1, edgecolor='black')\n",
    "ax.set_title(\"Classement des correlations des variables par rapport à la variable \\\"quality\\\"\", fontdict={\"size\":25})\n",
    "for i, v in enumerate(top[sorted.index].values):\n",
    "    ax.text(i - 0.25, (v + np.sign(v) * 0.015) - 0.01, f\"{round(v, 2):.2f}\", color='black')\n",
    "# plt.axhline(1, linestyle='--')\n",
    "# plt.axhline(-1, linestyle='--')\n",
    "plt.ylabel(\"Pearson correlation\", fontweight='light', fontsize='x-large')\n",
    "plt.xticks(rotation=67.5, horizontalalignment='right', fontweight='light', fontsize='large')\n",
    "plt.yticks(fontweight='light', fontsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# On ne considère pas la colonne type pour le moment qui est qualitative\n",
    "X = data.drop([Y_COL_NAME, 'type'], axis=1)\n",
    "# X = ((X - X.mean()) / X.std()).values # Standardization colonne par colonne # (x - µ) / σ\n",
    "# ou\n",
    "# Normalisation min-max\n",
    "def min_max_scaling(X):\n",
    "    return (X - X.min()) / (X.max() - X.min())\n",
    "\n",
    "# Réduction et centrage des valeurs autour de la moyenne\n",
    "def norm_scaling(X):\n",
    "    return ((X - X.mean()) / X.std())\n",
    "\n",
    "X = min_max_scaling(X)\n",
    "\n",
    "Y = data.loc[:, Y_COL_NAME].values\n",
    "NB_CLASS = len(np.unique(Y))\n",
    "Y = Y.reshape(-1, 1) # Y est scale entre 0 et 1 au moment de faire la regression (on garde les notes en entier pour le model de classif)\n",
    "\n",
    "NB_INPUT = X.shape[1] # nombre de variable en input des modeles\n",
    "NB_DATA = X.shape[0]\n",
    "# X, Y, X.shape, Y.shape\n",
    "\n",
    "# Split train/test\n",
    "X, X_test, Y, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X, X_test, Y, Y_test = X.values, X_test.values, Y - 3, Y_test - 3\n",
    "# Y_train = Y_train.reshape(-1)\n",
    "# Y_test = Y_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Modèles & Entrainments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Définition du réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.activations import *\n",
    "\n",
    "def classification_model(DROP_RATE = 0.1, RELU_ALPHA = 0.2, N_NEURONES = 32, N_LAYERS = 1):\n",
    "\t\n",
    "\tinput = Input(shape=(NB_INPUT,), name=\"input\")\n",
    "\tx = LayerNormalization()(input)\n",
    "\n",
    "\tfor _ in range(N_LAYERS):\n",
    "\t\tx = LeakyReLU(RELU_ALPHA)(BatchNormalization()(Dense(N_NEURONES)(x)))\n",
    "\t\tx = Dropout(DROP_RATE)(x)\n",
    "\t\n",
    "\toutput = Dense(NB_CLASS, activation=Softmax(), name=\"prediction\")(x)\n",
    "\t\n",
    "\treturn Model(input, output, name=\"classification_model\")\n",
    "\n",
    "# NOTE: Entrainement fait dans la CV plus bas\n",
    "# epochs = 50\n",
    "# lr = 1e-2\n",
    "# batch_size = 32\n",
    "\n",
    "# classModel = classification_model(DROP_RATE = 0.2, RELU_ALPHA = 0.2, N_NEURONES = 32, N_LAYERS = 3)\n",
    "# classModel.compile(loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=Adam(lr))\n",
    "# classModel.summary()\n",
    "#\n",
    "# hclass = classModel.fit(X, Y, validation_data=[X_test, Y_test], batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 / 10**(epoch/50) ) # \n",
    "# hclass = classModel.fit(X_train, Y_train, validation_data=[X_test, Y_test], batch_size=32, epochs=200, callbacks=[lr_schedule])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Définition des modèles de régression/classification et de la fonction d'entraîntement par KFold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, Lasso\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "\n",
    "def logistic_regression_model(random_state=0, multi_class='ovr', solver='liblinear', max_iter=100):\n",
    "    return LogisticRegression(random_state=random_state, multi_class=multi_class, solver=solver, max_iter=max_iter)\n",
    "\n",
    "def lasso_regression_model(random_state=0):\n",
    "    return Lasso(random_state=random_state)\n",
    "\n",
    "def ridge_classification_model():\n",
    "    return RidgeClassifier()\n",
    "\n",
    "def QDA_model():\n",
    "    return QuadraticDiscriminantAnalysis()\n",
    "\n",
    "def SVM_model(kernel='rbf', C=1, random_state=0, probability=False):\n",
    "    return SVC(kernel=kernel, C=C, random_state=random_state, probability=probability)\n",
    "\n",
    "def apply_model(model, X_train, X_test, Y_train, Y_test):\n",
    "\tmodel.fit(X_train, Y_train)\n",
    "\tprint('Précision du modèle : %0.2f' % model.score(X_test, Y_test))\n",
    "\treturn model\n",
    "    \n",
    "def apply_model_cv(model, X_train, Y_train, cv):\n",
    "\tscores = cross_val_score(model, X_train, Y_train, cv=cv, scoring='accuracy')\n",
    "\tprint(f\"\"\"\n",
    "\taccuracy: {np.round(scores*100, 2)}\n",
    "\tmean: {np.mean(scores*100):.2f}%\n",
    "\tecart-type: {np.std(scores*100):.2f}%\n",
    "\t\"\"\")\n",
    "        \n",
    "         \n",
    "# ######\n",
    "# import sklearn\n",
    "# sklearn.metrics.SCORERS.keys()\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Application des modèles et résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Modèle de régression logistique multinomiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n*** Test du modèle de régression logistique multinomiale avec CV ***\\n')\n",
    "logRegModel = logistic_regression_model(0, 'multinomial', 'lbfgs', 2000)\n",
    "apply_model_cv(logRegModel, X, Y.reshape(-1,), cv) # score cv\n",
    "logRegModel.fit(X, Y.reshape(-1,)) # train sur toute la base\n",
    "# Y_pred = logRegModel.predict(X_test)\n",
    "Y_pred = logRegModel.predict_proba(X_test) # shape => (-1, NB_CLASS)\n",
    "cce = np.mean(tf.keras.metrics.sparse_categorical_crossentropy(Y_test, Y_pred)) # Permet de comparer l'erreur du model à celui du deep\n",
    "print(f\"\"\"Categorical Crossentropy Error: {cce}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Modèle de classification Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\n*** Test du modèle ridge avec CV ***\\n')\n",
    "ridgeClf = ridge_classification_model()\n",
    "apply_model_cv(ridgeClf, X, Y.reshape(-1,), cv) # score cv\n",
    "ridgeClf.fit(X, Y.reshape(-1,)) # train sur toute la base\n",
    "Y_pred = ridgeClf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(Y_test + 3, Y_pred + 3, labels=np.arange(3, 10))\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt='g')\n",
    "plt.show()\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt='.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\n*** Test du modèle de régression logistique multinomiale avec CV ***\\n')\n",
    "# # lassoRegModel = lasso_regression_model()\n",
    "# lassoRegModel = Lasso(alpha=0.6, selection=\"random\", positive = True)\n",
    "# lassoRegModel.fit(X, Y.reshape(-1,)) # train sur toute la base\n",
    "# # lassoRegModel.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2]) # train sur toute la base\n",
    "# apply_model_cv(lassoRegModel, X, Y, cv) # score cv\n",
    "# Y_pred = lassoRegModel.predict(X_test)\n",
    "# # Y_pred = lassoRegModel.predict_proba(X_test) # shape => (-1, NB_CLASS)\n",
    "# # cce = np.mean(tf.keras.metrics.sparse_categorical_crossentropy(Y_test, Y_pred)) # Permet de comparer l'erreur du model à celui du deep\n",
    "# # print(f\"\"\"Categorical Crossentropy Error: {cce}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Modèle de régression SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n*** Test du modèle de régression logistique multinomiale avec CV ***\\n')\n",
    "svmModel = SVM_model(probability=True)\n",
    "apply_model_cv(svmModel, X, Y.reshape(-1,), cv) # score cv\n",
    "svmModel.fit(X, Y.reshape(-1,)) # train sur toute la base\n",
    "# Y_pred = logRegModel.predict(X_test)\n",
    "Y_pred = logRegModel.predict_proba(X_test) # shape => (-1, NB_CLASS)\n",
    "cce = np.mean(tf.keras.metrics.sparse_categorical_crossentropy(Y_test, Y_pred)) # Permet de comparer l'erreur du model à celui du deep\n",
    "print(f\"\"\"Categorical Crossentropy Error: {cce}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 1e-2\n",
    "batch_size = 32\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "hclass = []\n",
    "for fi, (train_i, val_i) in enumerate(cv.split(X, Y)):\n",
    "\tx_train, y_train = X[train_i], Y[train_i]\n",
    "\tx_val, y_val = X[val_i], Y[val_i]\n",
    "\n",
    "\tclassModel = classification_model(DROP_RATE = 0.2, RELU_ALPHA = 0.2, N_NEURONES = 32, N_LAYERS = 3)\n",
    "\tclassModel.compile(loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=Adam(lr))\n",
    "\thclass.append(classModel.fit(x_train, y_train, validation_data=[x_val, y_val], batch_size=batch_size, epochs=epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(hist, name=\"loss\"):\n",
    "\treturn np.array([h.history[name] for h in hist])\n",
    "loss = get_metrics(hclass, \"loss\")\n",
    "val_loss = get_metrics(hclass, \"val_loss\")\n",
    "# plt.plot(np.mean(loss, axis=0))\n",
    "# plt.plot(np.mean(val_loss, axis=0))\n",
    "plt.errorbar(x=range(loss.shape[1]), y=np.mean(loss, axis=0), yerr=np.std(loss, axis=0), label=\"loss\", fmt='o-')\n",
    "plt.errorbar(x=range(val_loss.shape[1]), y=np.mean(val_loss, axis=0), yerr=np.std(val_loss, axis=0), label=\"val_loss\", fmt='o-')\n",
    "plt.title(\"loss vs val_loss, CV: 5 folds\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\n*** Test de l\\'analyse discriminante quadratique ***\\n')\n",
    "qdaClf = QDA_model()\n",
    "apply_model(qdaClf, X, X_test, Y, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.array(hclass.history[\"val_loss\"]) / np.array(hclass.history[\"loss\"]), label=\"ratio test/train loss\")\n",
    "# plt.axhline(y=1, color='b', linestyle='--', label=\"Equilibre\")\n",
    "# plt.legend()\n",
    "# # Pour evaluer le surapprentissage (plus on s'eloigne de 1 en positif plus on est en surapprentisssage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.argmax(classModel.predict(X), axis=1)\n",
    "Y_true = Y\n",
    "cm = confusion_matrix(Y_true, Y_pred, labels=np.arange(0, 10))\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt='g')\n",
    "plt.show()\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt='.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.argmax(classModel.predict(X_test), axis=1)\n",
    "Y_true = Y_test\n",
    "cm = confusion_matrix(Y_true, Y_pred, labels=np.arange(0, 10))\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt='g')\n",
    "plt.show()\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 14}, fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Cross Val pour test different meta parametre\n",
    "# plot les resultats + leur std par epochs\n",
    "\n",
    "# TODO tester l'accuracy de la classif en faisant apparaitre aussi le second choix du model\n",
    "# possible aussi de regarder si les deux premier choix sont \"proche\" (cad si une prediction à 5 en choix 1 est suivit par un 4 ou 6 en choix 2 (faisable en faaisant mean(abs(choix1-choix2)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Conclure\n",
    "# TODO souligner les limites du dataset: que 3 votants, manque d'element en input comme le prix, les labels de qualité etc pour que le model soit pertinent dans le cadre d'une utilisation de classification de vin en situation de vente reel"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d30491a918b2bb80bddb8df1507fb40d67bf864fa8abf51b7fc41f1e6d55db91"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
